{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/50, Total Reward: 616, Total Score: 616\n",
      "Episode: 2/50, Total Reward: 608, Total Score: 608\n",
      "Episode: 3/50, Total Reward: 1248, Total Score: 1248\n",
      "Episode: 4/50, Total Reward: 540, Total Score: 540\n",
      "Episode: 5/50, Total Reward: 1048, Total Score: 1048\n",
      "Episode: 6/50, Total Reward: 648, Total Score: 648\n",
      "Episode: 7/50, Total Reward: 576, Total Score: 576\n",
      "Episode: 8/50, Total Reward: 564, Total Score: 564\n",
      "Episode: 9/50, Total Reward: 2160, Total Score: 2160\n",
      "Episode: 10/50, Total Reward: 1096, Total Score: 1096\n",
      "Episode: 11/50, Total Reward: 1064, Total Score: 1064\n",
      "Episode: 12/50, Total Reward: 988, Total Score: 988\n",
      "Episode: 13/50, Total Reward: 732, Total Score: 732\n",
      "Episode: 14/50, Total Reward: 1376, Total Score: 1376\n",
      "Episode: 15/50, Total Reward: 1960, Total Score: 1960\n",
      "Episode: 16/50, Total Reward: 512, Total Score: 512\n",
      "Episode: 17/50, Total Reward: 408, Total Score: 408\n",
      "Episode: 18/50, Total Reward: 564, Total Score: 564\n",
      "Episode: 19/50, Total Reward: 200, Total Score: 200\n",
      "Episode: 20/50, Total Reward: 844, Total Score: 844\n",
      "Episode: 21/50, Total Reward: 976, Total Score: 976\n",
      "Episode: 22/50, Total Reward: 1168, Total Score: 1168\n",
      "Episode: 23/50, Total Reward: 1312, Total Score: 1312\n",
      "Episode: 24/50, Total Reward: 2132, Total Score: 2132\n",
      "Episode: 25/50, Total Reward: 696, Total Score: 696\n",
      "Episode: 26/50, Total Reward: 1908, Total Score: 1908\n",
      "Episode: 27/50, Total Reward: 900, Total Score: 900\n",
      "Episode: 28/50, Total Reward: 1492, Total Score: 1492\n",
      "Episode: 29/50, Total Reward: 588, Total Score: 588\n",
      "Episode: 30/50, Total Reward: 904, Total Score: 904\n",
      "Episode: 31/50, Total Reward: 1504, Total Score: 1504\n",
      "Episode: 32/50, Total Reward: 1552, Total Score: 1552\n",
      "Episode: 33/50, Total Reward: 1432, Total Score: 1432\n",
      "Episode: 34/50, Total Reward: 1332, Total Score: 1332\n",
      "Episode: 35/50, Total Reward: 1040, Total Score: 1040\n",
      "Episode: 36/50, Total Reward: 1436, Total Score: 1436\n",
      "Episode: 37/50, Total Reward: 636, Total Score: 636\n",
      "Episode: 38/50, Total Reward: 872, Total Score: 872\n",
      "Episode: 39/50, Total Reward: 1320, Total Score: 1320\n",
      "Episode: 40/50, Total Reward: 504, Total Score: 504\n",
      "Episode: 41/50, Total Reward: 1344, Total Score: 1344\n",
      "Episode: 42/50, Total Reward: 552, Total Score: 552\n",
      "Episode: 43/50, Total Reward: 1352, Total Score: 1352\n",
      "Episode: 44/50, Total Reward: 788, Total Score: 788\n",
      "Episode: 45/50, Total Reward: 620, Total Score: 620\n",
      "Episode: 46/50, Total Reward: 588, Total Score: 588\n",
      "Episode: 47/50, Total Reward: 1492, Total Score: 1492\n",
      "Episode: 48/50, Total Reward: 884, Total Score: 884\n",
      "Episode: 49/50, Total Reward: 1088, Total Score: 1088\n",
      "Episode: 50/50, Total Reward: 2412, Total Score: 2412\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import tkinter as tk\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Fonction pour initialiser la grille avec deux tuiles au départ\n",
    "def initialize_game() -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Initialise une grille de jeu 4x4 avec deux tuiles (2 ou 4) placées aléatoirement.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Grille de jeu initialisée avec deux tuiles.\n",
    "    \"\"\"\n",
    "    board = [[0] * 4 for _ in range(4)]\n",
    "    add_new_tile(board)\n",
    "    add_new_tile(board)\n",
    "    return board\n",
    "\n",
    "# Fonction pour ajouter une nouvelle tuile (2 ou 4) à une position vide\n",
    "def add_new_tile(board: List[List[int]]) -> None:\n",
    "    \"\"\"\n",
    "    Ajoute une nouvelle tuile (2 ou 4) à une position vide dans la grille.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): La grille de jeu où ajouter une nouvelle tuile.\n",
    "    \"\"\"\n",
    "    empty_tiles = [(r, c) for r in range(4) for c in range(4) if board[r][c] == 0]\n",
    "    if empty_tiles:\n",
    "        r, c = random.choice(empty_tiles)\n",
    "        board[r][c] = 2 if random.random() < 0.85 else 4\n",
    "\n",
    "# Fonction pour fusionner une ligne ou une colonne de tuiles\n",
    "def merge(line: List[int]) -> Tuple[List[int], int]:\n",
    "    \"\"\"\n",
    "    Fusionne les tuiles identiques dans une ligne ou colonne, selon les règles du jeu.\n",
    "    Les tuiles adjacentes de même valeur fusionnent et le score est mis à jour en conséquence.\n",
    "\n",
    "    Args:\n",
    "        line (List[int]): Ligne ou colonne de tuiles à fusionner (une liste d'entiers).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], int]: La ligne fusionnée avec les tuiles à gauche, et le score total obtenu.\n",
    "    \"\"\"\n",
    "    non_zero = [num for num in line if num != 0]  # Supprime les zéros\n",
    "    merged = []\n",
    "    score = 0  # Score pour cette ligne/colonne\n",
    "    skip = False\n",
    "\n",
    "    for i in range(len(non_zero)):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if i + 1 < len(non_zero) and non_zero[i] == non_zero[i + 1]:\n",
    "            merged.append(non_zero[i] * 2)\n",
    "            score += non_zero[i] * 2  # Ajoute la valeur fusionnée au score\n",
    "            skip = True  # Saute la case suivante car elle a fusionné\n",
    "        else:\n",
    "            merged.append(non_zero[i])\n",
    "\n",
    "    # Complète avec des zéros à droite pour maintenir la taille de la ligne\n",
    "    merged.extend([0] * (len(line) - len(merged)))\n",
    "    return merged, score  # Retourne la ligne fusionnée et le score\n",
    "\n",
    "\n",
    "# Fonction pour déplacer et fusionner les tuiles à gauche\n",
    "def move_left(board: List[List[int]]) -> Tuple[List[List[int]], int]:\n",
    "    \"\"\"\n",
    "    Déplace et fusionne toutes les tuiles du plateau vers la gauche, en suivant les règles du jeu.\n",
    "    Calcule également le score obtenu suite aux fusions.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): Grille de jeu (4x4) avant le déplacement.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], int]: La grille mise à jour après le déplacement et le score total obtenu.\n",
    "    \"\"\"\n",
    "    score = 0  # Initialiser le score pour ce mouvement\n",
    "    for i in range(4):\n",
    "        new_row, gained_score = merge(board[i])  # Fusionner les tuiles et obtenir le score\n",
    "        board[i] = new_row  # Mettre à jour la ligne\n",
    "        score += gained_score  # Ajouter le score obtenu lors de la fusion\n",
    "    return board, score  # Retourner le plateau mis à jour et le score total\n",
    "\n",
    "# Fonction pour déplacer et fusionner les tuiles à droite\n",
    "def move_right(board: List[List[int]]) -> Tuple[List[List[int]], int]:\n",
    "    \"\"\"\n",
    "    Déplace et fusionne toutes les tuiles du plateau vers la droite.\n",
    "    Calcule le score total pour les fusions réalisées pendant le déplacement.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): Grille de jeu (4x4) avant le déplacement.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], int]: La grille mise à jour après le déplacement et le score total obtenu.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for i in range(4):\n",
    "        new_row, gained_score = merge(board[i][::-1])  # Fusionner après avoir inversé la ligne\n",
    "        board[i] = new_row[::-1]  # Réinverser après la fusion pour maintenir l'ordre\n",
    "        score += gained_score\n",
    "    return board, score\n",
    "\n",
    "# Fonction pour déplacer et fusionner les tuiles vers le haut\n",
    "def move_up(board: List[List[int]]) -> Tuple[List[List[int]], int]:\n",
    "    \"\"\"\n",
    "    Déplace et fusionne toutes les tuiles du plateau vers le haut.\n",
    "    Calcule le score pour les fusions réalisées lors du déplacement.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): Grille de jeu (4x4) avant le déplacement.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], int]: La grille mise à jour après le déplacement et le score total obtenu.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for col in range(4):\n",
    "        column = [board[row][col] for row in range(4)]  # Récupère la colonne actuelle\n",
    "        new_column, gained_score = merge(column)  # Fusionner les tuiles de la colonne\n",
    "        score += gained_score  # Ajouter le score de cette colonne\n",
    "        for row in range(4):\n",
    "            board[row][col] = new_column[row]  # Mettre à jour la colonne dans la grille\n",
    "    return board, score\n",
    "\n",
    "# Fonction pour déplacer et fusionner les tuiles vers le bas\n",
    "def move_down(board: List[List[int]]) -> Tuple[List[List[int]], int]:\n",
    "    \"\"\"\n",
    "    Déplace et fusionne toutes les tuiles du plateau vers le bas.\n",
    "    Calcule le score pour les fusions réalisées pendant le déplacement.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): Grille de jeu (4x4) avant le déplacement.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], int]: La grille mise à jour après le déplacement et le score total obtenu.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for col in range(4):\n",
    "        column = [board[row][col] for row in range(4)]  # Récupère la colonne actuelle\n",
    "        new_column, gained_score = merge(column[::-1])  # Fusionner après avoir inversé la colonne\n",
    "        score += gained_score  # Ajouter le score obtenu\n",
    "        for row in range(4):\n",
    "            board[row][col] = new_column[::-1][row]  # Réinverser pour revenir à l'ordre initial\n",
    "    return board, score\n",
    "\n",
    "\n",
    "class Game2048Env:\n",
    "    \"\"\"\n",
    "    Environnement du jeu 2048, gérant la logique de déplacement, fusion des tuiles et suivi du score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialise une instance de l'environnement du jeu 2048.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Réinitialise le plateau de jeu en créant un nouveau plateau avec deux tuiles initiales.\n",
    "        Réinitialise également le score total.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Le plateau de jeu (4x4) sous forme de tableau numpy.\n",
    "        \"\"\"\n",
    "        self.board = initialize_game()\n",
    "        self.total_score = 0  # Réinitialise le score total du jeu\n",
    "        return np.array(self.board)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, int, bool]:\n",
    "        \"\"\"\n",
    "        Effectue une action de déplacement dans la direction indiquée, met à jour le plateau,\n",
    "        ajoute une nouvelle tuile si le déplacement est valide, et met à jour le score.\n",
    "\n",
    "        Args:\n",
    "            action (int): L'action à exécuter (0: haut, 1: bas, 2: gauche, 3: droite).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, int, bool]: Le plateau de jeu mis à jour, le score obtenu sur ce mouvement, \n",
    "            et un booléen indiquant si la partie est terminée.\n",
    "        \"\"\"\n",
    "        prev_board = np.copy(self.board)\n",
    "        score = 0  # Initialiser le score pour ce mouvement\n",
    "\n",
    "        if action == 0:\n",
    "            self.board, score = move_up(self.board)\n",
    "        elif action == 1:\n",
    "            self.board, score = move_down(self.board)\n",
    "        elif action == 2:\n",
    "            self.board, score = move_left(self.board)\n",
    "        elif action == 3:\n",
    "            self.board, score = move_right(self.board)\n",
    "\n",
    "        if not np.array_equal(prev_board, self.board):\n",
    "            add_new_tile(self.board)  # Ajoute une nouvelle tuile si le plateau a changé\n",
    "\n",
    "        self.total_score += score  # Met à jour le score total\n",
    "\n",
    "        done = check_win(self.board) or check_game_over(self.board)  # Vérifie si la partie est terminée\n",
    "\n",
    "        return np.array(self.board), score, done\n",
    "    def get_score(self) -> int:\n",
    "        \"\"\"Retourne le score total actuel.\"\"\"\n",
    "        return self.total_score\n",
    "\n",
    "    def get_board(self) -> np.ndarray:\n",
    "        \"\"\"Retourne le plateau de jeu actuel.\"\"\"\n",
    "        return np.array(self.board)\n",
    "\n",
    "# Fonction pour vérifier si le joueur a atteint la tuile 2048 (condition de victoire)\n",
    "def check_win(board: List[List[int]]) -> bool:\n",
    "    \"\"\"\n",
    "    Vérifie si le joueur a gagné la partie en atteignant une tuile de valeur 2048.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): Le plateau de jeu.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si le joueur a gagné, sinon False.\n",
    "    \"\"\"\n",
    "    return any(2048 in row for row in board)\n",
    "\n",
    "# Fonction pour vérifier s'il reste des mouvements possibles (condition de défaite)\n",
    "def check_game_over(board: List[List[int]]) -> bool:\n",
    "    \"\"\"\n",
    "    Vérifie si la partie est terminée, c'est-à-dire s'il ne reste plus de mouvements possibles.\n",
    "\n",
    "    Args:\n",
    "        board (List[List[int]]): Le plateau de jeu.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si le jeu est terminé, sinon False.\n",
    "    \"\"\"\n",
    "    # Vérifie s'il reste des cases vides\n",
    "    if any(0 in row for row in board):\n",
    "        return False\n",
    "    # Vérifie les fusions possibles dans les lignes\n",
    "    for row in board:\n",
    "        for i in range(3):\n",
    "            if row[i] == row[i + 1]:\n",
    "                return False\n",
    "    # Vérifie les fusions possibles dans les colonnes\n",
    "    for col in range(4):\n",
    "        for i in range(3):\n",
    "            if board[i][col] == board[i + 1][col]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Classe DQNAgent pour l'entraînement par renforcement\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, model_file='dqn_model.keras', tau=1.0):\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.985\n",
    "        self.tau = max(tau, 1e-3) # Boltzmann temperature\n",
    "        self.learning_rate = 0.001  # Learning rate\n",
    "\n",
    "        # Création du modèle de Q-Network et du Target Network\n",
    "        self.model = self.create_model(input_shape, num_actions)\n",
    "        self.target_model = self.create_model(input_shape, num_actions)\n",
    "\n",
    "        # Fichiers pour sauvegarder la mémoire et les poids\n",
    "        self.model_file = model_file\n",
    "        \n",
    "        # Chargement de la mémoire et des poids si disponibles\n",
    "        self.load_model(self.model_file)\n",
    "        \n",
    "    def load_model(self, filename):\n",
    "        \"\"\"Charge un modèle depuis un fichier, si disponible.\"\"\"\n",
    "        if os.path.exists(filename):\n",
    "            self.model = tf.keras.models.load_model(filename)\n",
    "            # Recrée le modèle cible pour qu'il soit identique au modèle chargé\n",
    "            self.target_model = tf.keras.models.clone_model(self.model)\n",
    "            self.target_model.build(self.model.input_shape)  # Initialise le modèle avec la même forme d'entrée\n",
    "            self.update_target_model() # Synchronise les poids entre le modèle et le modèle cible\n",
    "        \n",
    "    def create_model(self, input_shape, num_actions):\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Input(shape=input_shape),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(num_actions, activation='linear')  # Output: Q-values for each action\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Met à jour le modèle cible avec les poids du modèle principal\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Sauvegarde la transition dans la mémoire\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    import tensorflow as tf\n",
    "\n",
    "    @staticmethod\n",
    "    def boltzmann_policy(q_values, tau=1.0):\n",
    "        # Avoid NaN by setting tau to a minimum threshold\n",
    "            tau = max(tau, 1e-3)\n",
    "            \n",
    "            # Calculate exponentials for Boltzmann probabilities\n",
    "            exp_q = np.exp(q_values / tau - np.max(q_values / tau))  # Subtract max for numerical stability\n",
    "            sum_exp_q = np.sum(exp_q)\n",
    "            \n",
    "            # Handle the case where sum_exp_q might be zero to avoid division by zero\n",
    "            if sum_exp_q == 0:\n",
    "                probs = np.ones(len(q_values)) / len(q_values)  # Assign uniform probabilities if all are zero\n",
    "            else:\n",
    "                probs = exp_q / sum_exp_q  # Calculate probabilities\n",
    "\n",
    "            # Ensure no NaNs in the final probabilities\n",
    "            probs = np.nan_to_num(probs, nan=1.0 / len(q_values))  # Replace NaNs with uniform probabilities\n",
    "            \n",
    "            return np.random.choice(len(q_values), p=probs)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Politique ε-greedy : explore avec probabilité epsilon, sinon exploite\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        q_values = self.model.predict(state, verbose=0)[0]\n",
    "        return self.boltzmann_policy(q_values, self.tau)\n",
    "        # q_values = self.model.predict(state, verbose=0)  # Prédit les Q-values pour chaque action\n",
    "        # return np.argmax(q_values[0])  # Choisit l'action avec la Q-value la plus élevée\n",
    "\n",
    "    def train(self):\n",
    "        # Sample a minibatch and perform prioritized experience replay if implemented\n",
    "        # Update model based on Q-learning or Double Q-learning\n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def replay(self):\n",
    "        if len(self.memory) < 64:  # Attends que la mémoire ait assez d'expériences\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, 64)  # Sélectionne un échantillon de la mémoire\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            if done:\n",
    "                target[0][action] = reward  # Récompense si l'épisode est terminé\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state, verbose=0)\n",
    "                target[0][action] = reward + self.gamma * np.amax(t[0])  # Q-value cible\n",
    "\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "        # Réduit epsilon (exploration) au fil du temps\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"Enregistre le modèle actuel dans un fichier.\"\"\"\n",
    "        #self.model.save(self.model_file)\n",
    "        self.model.save(\"dqn_model.keras\")\n",
    "            \n",
    "    def load_memory(self):\n",
    "        \"\"\"Charge la mémoire depuis un fichier.\"\"\"\n",
    "        memory_file = \"memory.pkl\"\n",
    "        if os.path.exists(memory_file):\n",
    "            with open(memory_file, \"rb\") as f:\n",
    "                self.memory = pickle.load(f)       \n",
    "    def save_memory(self):\n",
    "        \"\"\"Sauvegarde la mémoire dans un fichier.\"\"\"\n",
    "        with open(self.memory_file, 'wb') as f:\n",
    "            pickle.dump(self.memory, f)\n",
    "            \n",
    "            \n",
    "class Game2048EnvGUI(Game2048Env):\n",
    "    \"\"\"\n",
    "    Classe pour l'interface graphique du jeu 2048, héritant de Game2048Env.\n",
    "\n",
    "    Attributes:\n",
    "        root (tk.Tk): La fenêtre principale de l'application Tkinter.\n",
    "        canvas (tk.Canvas): Le canevas où le jeu est dessiné.\n",
    "        tiles (List[List[Tuple[int, int]]]): Liste contenant les identifiants des tuiles sur le canevas.\n",
    "        high_scores (List[dict]): Liste des meilleurs scores, où chaque score est associé à une date et un chiffre maximal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: tk.Tk) -> None:\n",
    "        \"\"\"\n",
    "        Initialise l'interface graphique du jeu 2048.\n",
    "\n",
    "        Args:\n",
    "            root (tk.Tk): La fenêtre principale de l'application Tkinter.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.root.title(\"2048 Training\")\n",
    "        # Définir une couleur de fond aléatoire\n",
    "        self.root.configure(bg=random.choice(['#F0E68C', '#ADD8E6', '#90EE90', '#FFB6C1', '#FFDEAD', '#FFE4B5']))\n",
    "        \n",
    "        self.canvas = tk.Canvas(root, width=400, height=400, bg='white')  # Couleur de fond blanche pour le canvas\n",
    "        self.canvas.pack()\n",
    "        \n",
    "        self.tiles = []  # Initialiser la liste des tuiles\n",
    "        for i in range(4):\n",
    "            row_tiles = []\n",
    "            for j in range(4):\n",
    "                x1, y1 = j * 100, i * 100\n",
    "                x2, y2 = x1 + 100, y1 + 100\n",
    "                # Créer un rectangle pour chaque case avec une bordure\n",
    "                tile = self.canvas.create_rectangle(x1, y1, x2, y2, fill='lightgrey', outline='black', width=2)\n",
    "                text = self.canvas.create_text(x1 + 50, y1 + 50, text='', font=(\"Helvetica\", 30))\n",
    "                row_tiles.append((tile, text))  # Stocke le rectangle et le texte ensemble\n",
    "            self.tiles.append(row_tiles)\n",
    "\n",
    "        self.high_scores: List[dict] = []  # Historique des meilleurs scores\n",
    "\n",
    "    def update_gui(self) -> None:\n",
    "        \"\"\"\n",
    "        Met à jour l'interface graphique pour refléter l'état actuel du plateau de jeu.\n",
    "        \"\"\"\n",
    "        tile_colors = {\n",
    "            0: \"#CDC1B4\",   # Couleur pour 0\n",
    "            2: \"#EEE4DA\",   # Couleur pour 2\n",
    "            4: \"#EDE0C8\",   # Couleur pour 4\n",
    "            8: \"#F2B179\",   # Couleur pour 8\n",
    "            16: \"#F59563\",  # Couleur pour 16\n",
    "            32: \"#F67C5F\",  # Couleur pour 32\n",
    "            64: \"#F67C5F\",  # Couleur pour 64\n",
    "            128: \"#EDCF72\", # Couleur pour 128\n",
    "            256: \"#EDCC61\", # Couleur pour 256\n",
    "            512: \"#EDC850\", # Couleur pour 512\n",
    "            1024: \"#EDC53F\",# Couleur pour 1024\n",
    "            2048: \"#EDC22E\",# Couleur pour 2048\n",
    "        }\n",
    "\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                value = self.board[i][j]\n",
    "                color = tile_colors.get(value, \"#CDC1B4\")  # Couleur par défaut pour les valeurs non définies\n",
    "                \n",
    "                # Mise à jour de la couleur et du texte des tuiles\n",
    "                tile, text = self.tiles[i][j]\n",
    "                self.canvas.itemconfig(tile, fill=color)  # Applique la couleur de fond à la tuile\n",
    "                self.canvas.itemconfig(text, text=str(value) if value != 0 else '')  # Met à jour le texte\n",
    "        \n",
    "        self.root.update()\n",
    "\n",
    "    def record_high_score(self, score: int, agent: DQNAgent) -> None:\n",
    "        \"\"\"\n",
    "        Enregistre le score et la case avec le plus haut chiffre.\n",
    "\n",
    "        Args:\n",
    "            score (int): Le score total réalisé dans l'épisode.\n",
    "        \"\"\"\n",
    "        # Recherche la case avec le plus haut chiffre\n",
    "        max_tile = max(max(row) for row in self.board)\n",
    "        num_layers = len(agent.model.layers)  # Nombre de couches du modèle\n",
    "        total_params = agent.model.count_params()  # Nombre total de paramètres\n",
    "        current_epsilon = agent.epsilon  # Valeur actuelle de epsilon\n",
    "        \n",
    "        high_score_entry = {\n",
    "            \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"score\": score,\n",
    "            \"max_tile\": max_tile,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"total_params\": total_params,\n",
    "            \"current_epsilon\": current_epsilon,\n",
    "            \"learning_rate\": agent.learning_rate,\n",
    "            \"gamma\": agent.gamma,\n",
    "        }\n",
    "        # Enregistre l'entrée dans un fichier\n",
    "        with open(\"high_scores.txt\", \"a\") as f:\n",
    "            f.write(f\"{high_score_entry['date']} - Score: {high_score_entry['score']}, Max Tile: {high_score_entry['max_tile']}, \"\n",
    "                    f\"Layers: {high_score_entry['num_layers']}, Params: {high_score_entry['total_params']}, \"\n",
    "                    f\"Epsilon: {high_score_entry['current_epsilon']}, \"\n",
    "                    f\"Learning Rate: {high_score_entry['learning_rate']}, \"\n",
    "                    f\"Gamma: {high_score_entry['gamma']}\\n\")\n",
    "\n",
    "        # # Ajouter le score à l'historique et garder les 10 meilleurs\n",
    "        # self.high_scores.append(high_score_entry)\n",
    "        # self.high_scores.sort(key=lambda x: x['score'], reverse=True)\n",
    "        # self.high_scores = self.high_scores[:10]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    env_gui = Game2048EnvGUI(root)\n",
    "    agent = DQNAgent(input_shape=(4, 4), num_actions=4)\n",
    "\n",
    "    episodes = 50  # Réduit pour visualiser rapidement\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env_gui.reset().reshape(1, 4, 4)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_score = 0  # Pour suivre le score total du jeu\n",
    "        #max_tile = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env_gui.step(action)\n",
    "            next_state = next_state.reshape(1, 4, 4)\n",
    "\n",
    "            total_reward += reward\n",
    "            total_score = env_gui.total_score  # Suivre le score total\n",
    "            #max_tile = env_gui.max_tile\n",
    "            \n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            env_gui.update_gui()  # Met à jour l'interface graphique avec le nouvel état du jeu\n",
    "            # time.sleep(0.5)  # Délai pour visualiser les actions\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode: {e + 1}/{episodes}, Total Reward: {total_reward}, Total Score: {total_score}\")\n",
    "                agent.update_target_model()\n",
    "                \n",
    "                # Enregistre le score total de cet épisode\n",
    "                env_gui.record_high_score(total_score, agent)\n",
    "                \n",
    "        agent.replay()\n",
    "\n",
    "        # Sauvegarder le modèle après chaque épisode\n",
    "        agent.save_model()  # Changer cette ligne pour appeler la sauvegarde du modèle\n",
    "\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0feec255",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Suppression de l'initialisation Tkinter\n",
    "    env = Game2048Env()  # Remplacer Game2048EnvGUI par Game2048Env\n",
    "    agent = DQNAgent(input_shape=(4, 4), num_actions=4)\n",
    "\n",
    "    episodes = 5  # Réduit pour visualiser rapidement\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset().reshape(1, 4, 4)  # Appel à reset de Game2048Env\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_score = 0  # Pour suivre le score total du jeu\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)  # Appel à step de Game2048Env\n",
    "            next_state = next_state.reshape(1, 4, 4)\n",
    "\n",
    "            total_reward += reward\n",
    "            total_score = env.total_score  # Suivre le score total\n",
    "            # max_tile = env.max_tile (si cette variable existe dans Game2048Env)\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Supprimer env_gui.update_gui() car il n'est plus nécessaire\n",
    "        agent.replay()\n",
    "\n",
    "        # Sauvegarder le modèle après chaque épisode\n",
    "        agent.save_model()  # Changer cette ligne pour appeler la sauvegarde du modèle\n",
    "\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
